---
title: "First Report"
author: "_Minh Chau Van Nguyen_"
output: 
 html_document:
    css: "styles.css"
    toc: TRUE
    toc_depth: 4
    toc_float: 
     smooth_scroll: TRUE
---

<style>

div.blue pre { background-color:lightblue; }
div.blue pre.r { background-color:lavender; }
</style>

<div class = "blue">

```{r echo = FALSE, message = FALSE, warning = FALSE}
source("library.R")
source("theme.R")
source("caption.R")
source("data.R")
```




******

<br>

## Family A orange data

The data used as an example throughout this report and the second report is from **Family A** from the **Northland** region. Family A has data recorded from 2013 til 2020, and we wish to compare between the predicted number of oranges and the true number of oranges for 2018. The purpose of this is to examine the realibility of the proposed Time Series models.  

<br>
<div style = "width:80%; height:auto; margin: auto;">
```{r echo = FALSE, purl = FALSE}
ts.A <- ts(familyA$Total, frequency = 12,
           start = c(2013, 1), end = c(2020, 12))
familyA <- familyA[!(names(familyA) %in% c("Region", "long", "lat", "X", "Name"))]
DT::datatable(familyA, rownames = FALSE, caption = "Table 1: Family A data summary",
               options = list(lengthMenu = c(5,10,15,20)))
```
</div>



<br>
<br>



## Analysis of the data
<br>
__<span style="background-color:#FFE1FF; font-size:15pt;">Aim : to forecast the number of oranges for 2018</span>__.

:::: {.blackbox data-latex=""}
::: {.center data-latex=""}
**Modelling Procedure**
:::
1. Plot the data.
2. If necessary, use a Box-Cox transformation to stabilize the variance.
3. If necessary, difference the data until appears stationary.
4. Plot the ACF/PACF of the differenced data and try to determine possible candidate models.
5. Run the chosen model and use AIC to select for a better model.
6. Check the residuals by plotting ACF and doing a portmanteau test of the residuals.
7. If the residuals look like white noise, calculate the forecasts. Otherwise, return to Step 4.
::::

<br>

<span style="font-size:13pt; font-style:italic; font-style:bold"> Step 1. Plot the data and identify any unusual observations</span>.

```{r  echo = FALSE, fig.asp = 0.5, fig.cap = capFig("Monthly Data Time Series Plot"), fig.align = "center", purl = FALSE}
ts.A <- ts(familyA$Total, frequency = 12,
           start = c(2013, 1), end = c(2019, 12))
autoplot(ts.A) + 
  geom_line(color = "#6495ED") +
  geom_point(color = "#6495ED") +
  my_theme() +
  ggtitle("Monthly oranges bought: Family A") + 
  xlab("Year") + ylab("Number of Oranges bought per month") +
  geom_vline(xintercept = 2013:2020, linetype = "dotted")
```

<br>

__<span style="background-color:#FFE1FF;">Comment</span>__: The data shows no particular trend, strong cyclic behavior and there seems to be an unusual pattern between 2015 and 2016. In addition, the mean is not constant, i.e. changes over time, implying non-stationarity. It might be sensible to remove data from 2013 up to 2015 to avoid invalid statistical results. 

<br>



```{r echo = FALSE, fig.asp = 0.5, fig.cap = capFig("New Time Series Monthly Data"), fig.align = "center", purl = FALSE}
new.tsA <- window(ts.A, start = c(2016, 1))

autoplot(new.tsA, series = "Real Data") + 
  geom_line(color = "#6495ED") +
  geom_point(color = "#6495ED") +
  my_theme() +
  ggtitle("Monthly oranges bought: Family A") + 
  xlab("Year") + ylab("Number of Oranges bought per month") +
  geom_vline(xintercept = 2016:2020, linetype = "dotted") +
  my_theme()
```

<br>
__<span style="background-color:#FFE1FF;">Comment</span>__: Since the data is recorded at monthly interval, we would expect to see some seasonility patterns in the time series plot. However, data was generated randomly which results in the data sort of behaving like white noise though non-stationarity is still strongly implied. 

```{r echo = FALSE, fig.asp = 0.5, fig.cap = capFig("New Time Series Monthly Data vs. Seasonally adjusted"), fig.align = "center", purl = FALSE}
seasadj.A <- seasadj(stl(new.tsA, s.window = "periodic"))

autoplot(new.tsA, series = "Real Data") + 
  autolayer(seasadj.A, series = "Seasonally Adjusted") + 
  guides(color = guide_legend(title = "Series")) +
  scale_color_manual(values = c("#7CFC00", "#6495ED")) + 
  xlab("Year") + ylab("") + 
  ggtitle("Real data vs. Seasonally adjusted data") +
  my_theme()
```

>White noise is an important concept in time series analysis and forecasting. It is important for two main reasons:
**Predictability**: If your time series is white noise, then, by definition, it is random. You cannot reasonably model it and make predictions.
**Model Diagnostics**: The series of errors from a time series forecast model should ideally be white noise.

<br>
```{r echo = FALSE, fig.asp = 0.5, fig.cap = capFig("STL decompostion of additive components"), fig.align = "center", purl = FALSE}
new.tsA %>% stl(t.window = 13, s.window = "periodic", robust = TRUE) %>%
  autoplot() + geom_line(color = "#6495ED") + 
  my_theme()
```

<br>

The decompositon of the Family A data uses the __STL__ method. STL is an acronym for _“Seasonal and Trend decomposition using Loess”_, while Loess is a method for estimating non-linear relationships. It only provides facilities for additive decompositions. 

```{r echo = FALSE, purl = FALSE, fig.cap = capFig("ACF plot of the Tasman data"), fig.asp = 0.5, fig.align = "center", fig.width = 4}
ci0 <- qnorm((1 + 0.95)/2)/sqrt(length(new.tsA))
ggAcf(new.tsA) +
  geom_segment(lineend = "butt", color = "#6495ED") +
  geom_hline(yintercept = 0, color = "#6495ED") +
  geom_hline(yintercept = c(ci0, -ci0), color = "#FFA500", linetype = "dashed") +
  my_theme()
```

<br>
__<span style="background-color:#FFE1FF;">Comment</span>__: In the above ACF plot, the dashed orange lines indicates whether the autocorrelations are (statistically) significantly different from zero within 95% confidence limits. Here the autocorrelations are significantly different from 0, indicating high autocorrelation.  

<br>

<span style="font-size:13pt; font-style:italic; font-style:bold"> Step 2. Split the data into training and test sets</span>.

The accuracy of forecasts can only be determined by considering how well a model performs on new data that were not used when fitting the model. <span style="background-color:#FFE1FF"> When choosing models, it is common practice to separate the available data into two portions, training and test sets, where the training data is used to fit a forecasting method and the test data is used to evaluate its accuracy.</span> Because the test data is not used in determining the forecasts, it should provide a reliable indication of how well the model is likely to forecast on new data.


```{r}
# The data is split into training set (Jan,2016-Dec,2017) and test set (2018).
training <- window(new.tsA, start = c(2016,1), end = c(2017,12))
test <- window(new.tsA, start = c(2018,1), end = c(2018, 12))
```



<br>
<span style="font-size:13pt; font-style:italic; font-style:bold"> Step 3. Transform the data from the training set</span>.

__Box-Cox transformations__ is a family of transformations, that includes both logarithms and power transformations, used to transform the Family A data. This is recommended as the __Hyndman-Khandakar__ algorithm of the ```auto.arima()``` function only takes care of step 3-5 thus we still have to do steps 1, 2, 6 and 7 manually  to ensure the residuals will be roughly homoscedastic. One important feature of __power transformation__ is the $\lambda$ parameter, where $\lambda = 0$ is equivalent to a log-transformation. A good value of $\lambda$ is one which makes the size of the seasonal variation about the same across the whole series, as that makes the forecasting model simpler. The ```BoxCox.lambda()``` function can be used for choosing $\lambda$ automatically instead of doing it manually. 


```{r, purl = FALSE}
lambda <- BoxCox.lambda(training)
trans.A <- BoxCox(training, lambda)
lambda
```

<br>
The optimal value of $\lambda$ is 1, this is equivalent to $Y^{\lambda} = Y^1 = Y$ hence data transformation is not necessary. _Note_: If transformation was to be required, the data must be transformed AFTER being split into training and test sets in order to avoid data leakage, that is only the training data is transformed and not the test data.

<br>
<span style="font-size:13pt; font-style:italic; font-style:bold"> Step 4. Fit the models</span>.
In this step, two methods were considered to fit and predict the data. The methods used are ARIMA (```auto.arima()```) and ETS (```ets()```) models. 

<br>

#### METHOD 1: ARIMA MODEL

The ```auto.arima()``` function in R  uses an algorithm which combines unit root tests, minimisation of the AICc and MLE to obtain an ARIMA model. The arguments ```stepwise = FALSE``` and ```approximation = FALSE```are included in the below ```auto.arima()``` function to ensure ALL fitted seasonal ARIMA models are considered. The ARIMA model overall has the form : $$ARIMA(p,d,q)(P,D,Q)[m]$$, where ```p``` is the order of the _Autoregressive model_, ```d``` is the order of differencing and ```q``` is the order of the _Moving Average model_. ```(P,D,Q)``` is the same but are defined in the context of seasonality;. The final chosen model returned then has the form : $ARIMA(1,0,0)$. 


```{r, purl = FALSE, warning = FALSE, message = FALSE}
auto.arima.model <- auto.arima(training, stepwise = FALSE, approximation = FALSE)
auto.arima.model
```


<br>

__<span style="background-color:#FFE1FF;">Comment</span>__: An ARIMA(0,0,0) model with zero mean is white noise, so it means that the errors are uncorrelated across time.

<br>

```{r echo = FALSE, message = FALSE, purl = FALSE, fig.width = 8, fig.height = 3, fig.align = "center"}
autoplot(resid(auto.arima.model)) +
  geom_line(color = "#6495ED") +
  my_theme() +
  ggtitle("Residuals of ARIMA(0,0,0)") + 
  xlab("Year") + ylab("") 
```


```{r echo = FALSE, purl = FALSE, message = FALSE, warning = FALSE, fig.width = 8, fig.height = 3, fig.ncol = 1, fig.cap = capFig("Residuals of ARIMA(0,0,0) model"), fig.align = "center"}
ci1 <- qnorm((1 + 0.95)/2)/sqrt(length(resid(auto.arima.model)))
ggAcf(resid(auto.arima.model)) +
  geom_segment(lineend = "butt", color = "#6495ED") +
  geom_hline(yintercept = 0, color = "#6495ED") +
  geom_hline(yintercept = c(ci1, -ci1), color = "#FFA500", linetype = "dashed") +
  my_theme(panel.border =  element_rect(fill = NA, color = "#00CC99", linetype = "solid")) -> p3

update_geom_defaults("line", list(color = "darkblue"))
update_geom_defaults("bar", list(fill = "#6495ED"))
gghistogram(resid(auto.arima.model), add.rug = TRUE, add.normal = TRUE) + ggtitle("Histogram of residuals") + my_theme() + geom_line() -> p4

grid.arrange(p3, p4, ncol = 2)

Box.test(resid(auto.arima.model), type = "Lj", lag = 12, fitdf = 0)
```

The ACF plot of the residuals from the chosen ARIMA model shows that all autocorrelations are within the threshold limits indicating that the residuals are behaving like white noise. The histogram suggests that the residuals may not follow a Normal distribution. The Box-Ljung test returns a large p-value (p-value = 0.2184), also suggesting that the residuals resemble white noise. 

<br>

#### METHOD 2 : ETS MODEL
In the case of ETS (Error, Trend, Seasonal) models, the ```ets()``` function can be used to fit these types of model. The notation for each component is defined as Error = {A,M}, Trend = {N,A,Ad} and Seasonal = {N,A,M}, where _A_ stands for _additive_ and _M_ stands for _multiplicative_. 

```{r purl = FALSE}
ets.model <- ets(training)
ets.model
```

The best ETS model selected is the $ETS(A,N,N)$, as shown in the result above. Formally, this model is also known as the simple smoothing with additive errors model. In comparison to the $ARIMA(0,0,0)$ model, the ETS model has a <span style="background-color:#FFE1FF;">higher AIC value of 215.2654 (the ARIMA model's AIC value is 205.1)</span>.

```{r echo = FALSE, message = FALSE, purl = FALSE, fig.width = 8, fig.height = 3, fig.align = "center"}
autoplot(resid(ets.model)) +
  geom_line(color = "#6495ED") +
  my_theme() +
  ggtitle("Residuals of ETS(A,N,N)") + 
  xlab("Year") + ylab("") 
```


```{r echo = FALSE, message = FALSE, purl = FALSE, warning = FALSE, fig.width = 8, fig.height = 3, fig.ncol = 1, fig.cap = capFig("Residuals of ETS(A,N,A) model"), fig.align = "center"}
ci2 <- qnorm((1 + 0.95)/2)/sqrt(length(resid(ets.model)))
ggAcf(resid(ets.model)) +
  geom_segment(lineend = "butt", color = "#6495ED") +
  geom_hline(yintercept = 0, color = "#6495ED") +
  geom_hline(yintercept = c(ci2, -ci2), color = "#FFA500", linetype = "dashed") +
  my_theme() -> p5

update_geom_defaults("line", list(color = "darkblue"))
update_geom_defaults("bar", list(fill = "#6495ED"))
gghistogram(resid(ets.model), add.rug = TRUE, add.normal = TRUE) + ggtitle("Histogram of residuals") + my_theme() + geom_line() -> p6
grid.arrange(p5, p6, ncol = 2)

Box.test(resid(ets.model), type = "Lj", lag = 14, fitdf = 0)
```


The ACF plot of the residuals from the chosen ETS model shows that all autocorrelations are within the threshold limits indicating that the residuals are bot behaving like white noise. The histogram suggests that the residuals don't follow a Normal distribution.  The Box-Ljung test returns a large p-value (<span style="background-color:#FFE1FF;">p-value = 0.2676</span>), again suggesting that the residuals resemble white noise. 

<br>

<span style="font-size:13pt; font-style:italic; font-style:bold"> Step 5. Forecast the data using the fitted models</span>.

>Only one thing is true about forecasts - they are always wrong.

The ```forecast()``` function can be implemented in order to obtain forecasts from an ARIMA or an ETS model. Having done a transformation on the data, it is necessary to reverse the transformation (or back-transform) when forecasting the transformed data to obtain forecasts on the original scale. This can be done in R by adding the ```$\lambda$``` (equal to the $\lambda$ value selected for transforming the data) argument to the ```forecast()``` function. In addition, the ```biasadj = TRUE``` argument indicates that the mean of the forecasts is used and this mean is biased, whereas when ```biasadj = FALSE``` (default) the median of the forecasts is used and is not biased.

<br>

##### ARIMA(1,0,0) MODEL

```{r echo = FALSE, message = FALSE, purl = FALSE, warning = FALSE, fig.asp = 0.5, fig.cap = capFig("Forecasts for the seasonally adjusted Tasman data"), fig.align = "center"}
source("dygraph.R")

ARIMA.mean <- forecast(auto.arima.model, level = c(30,50,70), h = 12, biasadj = TRUE)

dygraph.func(mode = ARIMA.mean, main = paste("Forecast from", ARIMA.mean$method))
```

<br>

##### ETS(A,N,A) MODEL

```{r echo = FALSE, message = FALSE, warning = FALSE, purl = FALSE, fig.asp = 0.5, fig.cap = capFig("Forecasts for the seasonally adjusted Tasman data"), fig.align = "center"}
ETS.mean <- forecast(ets.model, h = 12, biasadj = TRUE, level = c(30,50,70), lambda = lambda)

dygraph.func(model = ETS.mean, main = paste("Forecast from", ETS.mean$model$method))
```

<br>

## ARIMA VS. ETS 

While linear exponential smoothing models are all special cases of ARIMA models, the non-linear exponential smoothing models have no equivalent ARIMA counterparts. On the other hand, there are also many ARIMA models that have no exponential smoothing counterparts. In particular, all ETS models are non-stationary, while some ARIMA models are stationary.

```{r}
a1 <- auto.arima.model %>% forecast(h = 12) %>% 
  accuracy(test)
a1[,c("RMSE", "MAE", "MAPE", "MASE")]
```

```{r}
a2 <- ets.model %>% forecast(h = 12) %>% 
  accuracy(test)
a2[,c("RMSE", "MAE", "MAPE", "MASE")]
```

<br>
__<span style="background-color:#FFE1FF;">Comment</span>__: The R output shows similar error values regarding the forecasting performance of the two competing models over the test data.



```{r echo = FALSE, message = FALSE, warning = FALSE, purl = FALSE, results = 'asis'}
require(pander)
panderOptions('table.split.table', Inf)
set.caption("Table 2: True values vs. Predicted values")
my.data <- "
     2018 | True Data   | Predicted ARIMA | Predicted ETS 
     Jan  | 10      | 27           | 27        
     Feb  | 42      | 27           | 27          
     Mar  | 8      | 27          | 27        
     Apr  | 37        | 27          | 27        
     May  | 31        | 27           | 27          
     Jun  | 19        | 27             | 27          
     Jul  | 16        | 27             | 27          
     Aug  | 45       | 27             | 27          
     Sep  | 14        | 27             | 27          
     Oct  | 43        | 27           | 27          
     Nov  | 33      | 27           | 27        
     Dec  | 9      | 27           | 27        "

df <- read.delim(textConnection(my.data),header = FALSE, sep = "|", strip.white = TRUE, stringsAsFactors = FALSE)
names(df) <- unname(as.list(df[1,])) # put headers on
df <- df[-1,] # remove first row
row.names(df) <- NULL
pander(df, style = 'rmarkdown')
```

<br>

__<span style="background-color:#FFE1FF;">Comment</span>__: Since the data appears to behave like a white noise series, it was expected that the forecasts didn't come out too well. Though the errors of both models were small, this also indicates that both models might be over-fitting the data. 

 
<br><br>

## To-Do List 
:::: {.blackbox data-latex=""}
<ul>
<li><input type="checkbox"> Perform time series analysis on the data sets basing on Region levels.</li>

<li><input type="checkbox"> Decompose annual GDP by region into monthly data.</li>

<li><input type="checkbox"> Dummy seasonal (monthly) predictors.</li>

<li><input type="checkbox" checked> Look into other forecasting methods, such as __<span style="color:#000066;">STL-ETS</span>__, __<span style="color:#000066;">NNAR</span>__, __<span style="color:#000066;">TBATS</span>__ and __<span style="color:#000066;">forecast combinations</span>__ (using several different methods on the same time series, and to average the resulting forecasts).</li>
</ul>
::::

<br><br>

## Reference

https://otexts.com/fpp2/

http://course1.winona.edu/bdeppa/FIN%20335/Handouts/Forecasters_Toolbox__Section_2_and_3.html#example-2---monthly-fastenal-sales-and-number-of-business-days-contd

https://stackoverflow.com/questions/59672182/prediction-interval-level-on-forecast-autoplot

<br>

```{r, child="_page_built_on.Rmd"}
```